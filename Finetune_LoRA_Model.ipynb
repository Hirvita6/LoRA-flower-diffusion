{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-10T17:24:56.050601Z",
          "iopub.status.busy": "2025-04-10T17:24:56.050419Z",
          "iopub.status.idle": "2025-04-10T17:25:18.734730Z",
          "shell.execute_reply": "2025-04-10T17:25:18.734039Z",
          "shell.execute_reply.started": "2025-04-10T17:24:56.050583Z"
        },
        "id": "keKDLIkXKAK1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/huggingface/diffusers\n",
        "%cd /kaggle/working/diffusers\n",
        "!pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-04-10T17:25:18.735971Z",
          "iopub.status.busy": "2025-04-10T17:25:18.735705Z",
          "iopub.status.idle": "2025-04-10T17:25:18.741509Z",
          "shell.execute_reply": "2025-04-10T17:25:18.740703Z",
          "shell.execute_reply.started": "2025-04-10T17:25:18.735926Z"
        },
        "id": "8FkEHjYkLVJK",
        "outputId": "1cf9f9db-d344-4f1f-ac5e-fd873d253cd1",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/kaggle/working/diffusers/examples/text_to_image\n"
          ]
        }
      ],
      "source": [
        "%cd /kaggle/working/diffusers/examples/text_to_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-10T17:25:18.742449Z",
          "iopub.status.busy": "2025-04-10T17:25:18.742275Z",
          "iopub.status.idle": "2025-04-10T17:25:22.991614Z",
          "shell.execute_reply": "2025-04-10T17:25:22.990826Z",
          "shell.execute_reply.started": "2025-04-10T17:25:18.742433Z"
        },
        "id": "-vtwJKV3KIFu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-10T17:25:22.994194Z",
          "iopub.status.busy": "2025-04-10T17:25:22.993965Z",
          "iopub.status.idle": "2025-04-10T17:25:26.299968Z",
          "shell.execute_reply": "2025-04-10T17:25:26.298984Z",
          "shell.execute_reply.started": "2025-04-10T17:25:22.994173Z"
        },
        "id": "ILrAaJPWKQTA",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install diffusers[torch] accelerate peft>=0.6.0 transformers datasets safetensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-10T17:25:26.303479Z",
          "iopub.status.busy": "2025-04-10T17:25:26.303291Z",
          "iopub.status.idle": "2025-04-10T17:25:41.440886Z",
          "shell.execute_reply": "2025-04-10T17:25:41.440173Z",
          "shell.execute_reply.started": "2025-04-10T17:25:26.303454Z"
        },
        "id": "_fARgHg7Lcja",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!accelerate config default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-10T17:25:41.442143Z",
          "iopub.status.busy": "2025-04-10T17:25:41.441885Z",
          "iopub.status.idle": "2025-04-10T17:25:41.965464Z",
          "shell.execute_reply": "2025-04-10T17:25:41.964861Z",
          "shell.execute_reply.started": "2025-04-10T17:25:41.442119Z"
        },
        "id": "zjiFh2-XLiSN",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Paste your token here (you can generate one at https://huggingface.co/settings/tokens)\n",
        "login(\"Huggingface Hub Token\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-10T17:25:41.966394Z",
          "iopub.status.busy": "2025-04-10T17:25:41.966156Z",
          "iopub.status.idle": "2025-04-10T17:50:26.975053Z",
          "shell.execute_reply": "2025-04-10T17:50:26.974125Z",
          "shell.execute_reply.started": "2025-04-10T17:25:41.966372Z"
        },
        "id": "tWuMzimIcVOA",
        "outputId": "1345e53d-22d9-481d-df06-ae85203f31a0",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-04-10 17:25:55.857880: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-04-10 17:25:55.857879: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744305956.100842     145 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744305956.100853     144 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744305956.166583     144 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "E0000 00:00:1744305956.166593     145 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "scheduler_config.json: 100%|███████████████████| 313/313 [00:00<00:00, 2.92MB/s]\n",
            "tokenizer_config.json: 100%|███████████████████| 806/806 [00:00<00:00, 8.33MB/s]\n",
            "vocab.json:   0%|                                   | 0.00/1.06M [00:00<?, ?B/s]{'clip_sample_range', 'sample_max_value', 'variance_type', 'dynamic_thresholding_ratio', 'rescale_betas_zero_snr', 'prediction_type', 'timestep_spacing', 'thresholding'} was not found in config. Values will be initialized to default values.\n",
            "vocab.json: 100%|██████████████████████████| 1.06M/1.06M [00:00<00:00, 15.8MB/s]\n",
            "merges.txt: 100%|████████████████████████████| 525k/525k [00:00<00:00, 30.5MB/s]\n",
            "special_tokens_map.json: 100%|█████████████████| 472/472 [00:00<00:00, 5.61MB/s]\n",
            "config.json: 100%|█████████████████████████████| 592/592 [00:00<00:00, 3.97MB/s]\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "model.safetensors: 100%|██████████████████████| 492M/492M [00:02<00:00, 223MB/s]\n",
            "config.json: 100%|█████████████████████████████| 551/551 [00:00<00:00, 4.87MB/s]\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "diffusion_pytorch_model.safetensors: 100%|████| 335M/335M [00:01<00:00, 247MB/s]\n",
            "{'use_post_quant_conv', 'norm_num_groups', 'use_quant_conv', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at CompVis/stable-diffusion-v1-4.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "config.json: 100%|█████████████████████████████| 743/743 [00:00<00:00, 6.28MB/s]\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "diffusion_pytorch_model.safetensors: 100%|██| 3.44G/3.44G [00:20<00:00, 170MB/s]\n",
            "{'time_cond_proj_dim', 'mid_block_only_cross_attention', 'transformer_layers_per_block', 'only_cross_attention', 'resnet_out_scale_factor', 'conv_out_kernel', 'encoder_hid_dim_type', 'use_linear_projection', 'class_embed_type', 'dropout', 'projection_class_embeddings_input_dim', 'conv_in_kernel', 'resnet_skip_time_act', 'class_embeddings_concat', 'encoder_hid_dim', 'resnet_time_scale_shift', 'attention_type', 'num_attention_heads', 'cross_attention_norm', 'num_class_embeds', 'time_embedding_act_fn', 'mid_block_type', 'time_embedding_dim', 'timestep_post_act', 'addition_embed_type_num_heads', 'addition_embed_type', 'addition_time_embed_dim', 'upcast_attention', 'dual_cross_attention', 'reverse_transformer_layers_per_block', 'time_embedding_type'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
            "\n",
            "All the weights of UNet2DConditionModel were initialized from the model checkpoint at CompVis/stable-diffusion-v1-4.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
            "README.md: 100%|███████████████████████████████| 425/425 [00:00<00:00, 3.21MB/s]\n",
            "train-00000-of-00001.parquet: 100%|████████| 28.0M/28.0M [00:00<00:00, 87.7MB/s]\n",
            "test-00000-of-00001.parquet: 100%|██████████| 2.73M/2.73M [00:00<00:00, 151MB/s]\n",
            "Generating train split: 100%|███████████| 91/91 [00:00<00:00, 313.36 examples/s]\n",
            "Generating test split: 100%|█████████████| 9/9 [00:00<00:00, 1077.21 examples/s]\n",
            "[rank1]:[W410 17:26:39.684869187 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
            "[rank0]:[W410 17:26:40.427566087 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
            "Steps:   3%|▏     | 46/1380 [00:13<05:39,  3.93it/s, lr=0.0001, step_loss=0.385]\n",
            "model_index.json: 100%|████████████████████████| 541/541 [00:00<00:00, 4.60MB/s]\u001b[A\n",
            "\n",
            "Fetching 14 files:   0%|                                 | 0/14 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "preprocessor_config.json: 100%|████████████████| 342/342 [00:00<00:00, 2.80MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Fetching 14 files:   7%|█▊                       | 1/14 [00:00<00:01,  9.98it/s]\u001b[A\n",
            "\n",
            "scheduler_config-checkpoint.json: 100%|████████| 209/209 [00:00<00:00, 2.80MB/s]\u001b[A\u001b[A\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "\n",
            "\n",
            "config.json: 100%|█████████████████████████| 4.56k/4.56k [00:00<00:00, 30.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Fetching 14 files:  21%|█████▎                   | 3/14 [00:00<00:00, 14.87it/s]\u001b[A\n",
            "\n",
            "model.safetensors:   0%|                            | 0.00/1.22G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:   1%|▏                   | 10.5M/1.22G [00:00<00:11, 102MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:   3%|▌                   | 31.5M/1.22G [00:00<00:08, 145MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:   5%|█                   | 62.9M/1.22G [00:00<00:05, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:   8%|█▌                  | 94.4M/1.22G [00:00<00:05, 218MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  10%|██▏                  | 126M/1.22G [00:00<00:04, 227MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  13%|██▋                  | 157M/1.22G [00:00<00:04, 233MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  16%|███▎                 | 189M/1.22G [00:00<00:04, 232MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  18%|███▊                 | 220M/1.22G [00:00<00:04, 237MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  21%|████▎                | 252M/1.22G [00:01<00:04, 238MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  23%|████▉                | 283M/1.22G [00:01<00:03, 239MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  26%|█████▍               | 315M/1.22G [00:01<00:03, 236MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  28%|█████▉               | 346M/1.22G [00:01<00:03, 239MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  31%|██████▌              | 377M/1.22G [00:01<00:03, 231MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  34%|███████              | 409M/1.22G [00:01<00:03, 234MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  36%|███████▌             | 440M/1.22G [00:01<00:03, 239MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  39%|████████▏            | 472M/1.22G [00:02<00:03, 237MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  41%|████████▋            | 503M/1.22G [00:02<00:03, 233MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  44%|█████████▏           | 535M/1.22G [00:02<00:02, 242MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  47%|█████████▊           | 566M/1.22G [00:02<00:02, 244MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  49%|██████████▎          | 598M/1.22G [00:02<00:02, 249MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  52%|██████████▊          | 629M/1.22G [00:02<00:02, 250MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  54%|███████████▍         | 661M/1.22G [00:02<00:02, 251MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  57%|███████████▉         | 692M/1.22G [00:02<00:02, 252MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  60%|████████████▍        | 724M/1.22G [00:03<00:01, 256MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  62%|█████████████        | 755M/1.22G [00:03<00:01, 244MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  65%|█████████████▌       | 786M/1.22G [00:03<00:01, 252MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  67%|██████████████       | 818M/1.22G [00:03<00:01, 249MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  70%|██████████████▋      | 849M/1.22G [00:03<00:01, 254MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  72%|███████████████▏     | 881M/1.22G [00:03<00:01, 247MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  75%|███████████████▊     | 912M/1.22G [00:03<00:01, 250MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  78%|████████████████▎    | 944M/1.22G [00:03<00:01, 247MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  80%|████████████████▊    | 975M/1.22G [00:04<00:00, 249MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  83%|████████████████▌   | 1.01G/1.22G [00:04<00:00, 250MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  85%|█████████████████   | 1.04G/1.22G [00:04<00:00, 258MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  88%|█████████████████▌  | 1.07G/1.22G [00:04<00:00, 259MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  91%|██████████████████  | 1.10G/1.22G [00:04<00:00, 258MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  93%|██████████████████▋ | 1.13G/1.22G [00:04<00:00, 256MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  96%|███████████████████▏| 1.16G/1.22G [00:04<00:00, 247MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors: 100%|████████████████████| 1.22G/1.22G [00:05<00:00, 241MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Fetching 14 files: 100%|████████████████████████| 14/14 [00:05<00:00,  2.64it/s]\u001b[A\n",
            "{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 12.75it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00,  9.13it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'norm_num_groups', 'use_quant_conv', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 14.13it/s]\n",
            "Steps:   7%|▍     | 92/1380 [00:59<05:47,  3.71it/s, lr=0.0001, step_loss=0.936]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 14.62it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00,  9.95it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'norm_num_groups', 'use_quant_conv', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 16.55it/s]\n",
            "Steps:  10%|▍   | 138/1380 [01:42<05:54,  3.51it/s, lr=0.0001, step_loss=0.0318]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 14.46it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00,  9.96it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'norm_num_groups', 'use_quant_conv', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 16.57it/s]\n",
            "Steps:  13%|▊     | 184/1380 [02:28<06:10,  3.23it/s, lr=0.0001, step_loss=0.29]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 14.55it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00, 10.11it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'norm_num_groups', 'use_quant_conv', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 16.76it/s]\n",
            "Steps:  17%|▊    | 230/1380 [03:15<05:41,  3.37it/s, lr=0.0001, step_loss=0.128]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 14.02it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00,  9.93it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'norm_num_groups', 'use_quant_conv', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 16.46it/s]\n",
            "Steps:  20%|▊   | 276/1380 [04:01<05:32,  3.32it/s, lr=0.0001, step_loss=0.0511]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 15.05it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00, 10.07it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'norm_num_groups', 'use_quant_conv', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 16.79it/s]\n",
            "Steps:  23%|█▏   | 322/1380 [04:47<05:15,  3.35it/s, lr=0.0001, step_loss=0.254]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 14.28it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00,  9.89it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'norm_num_groups', 'use_quant_conv', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 16.49it/s]\n",
            "Steps:  27%|█   | 368/1380 [05:33<05:04,  3.33it/s, lr=0.0001, step_loss=0.0948]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 15.16it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00,  9.86it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'norm_num_groups', 'use_quant_conv', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 16.41it/s]\n",
            "Steps:  30%|█▌   | 414/1380 [06:20<04:50,  3.33it/s, lr=0.0001, step_loss=0.487]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 14.95it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00, 10.14it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'norm_num_groups', 'use_quant_conv', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 16.88it/s]\n",
            "Steps:  33%|█  | 460/1380 [07:06<04:37,  3.32it/s, lr=0.0001, step_loss=0.00414]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 13.89it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00,  9.75it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'norm_num_groups', 'use_quant_conv', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 16.14it/s]\n",
            "Steps:  37%|█▊   | 506/1380 [07:53<04:21,  3.35it/s, lr=0.0001, step_loss=0.111]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 14.24it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00,  9.77it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'norm_num_groups', 'use_quant_conv', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 16.31it/s]\n",
            "Steps:  40%|██   | 552/1380 [08:39<04:09,  3.32it/s, lr=0.0001, step_loss=0.192]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 15.34it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00,  9.92it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'norm_num_groups', 'use_quant_conv', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 16.57it/s]\n",
            "Steps:  43%|█▋  | 598/1380 [09:25<03:53,  3.34it/s, lr=0.0001, step_loss=0.0773]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 11.67it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00,  9.06it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'norm_num_groups', 'use_quant_conv', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 14.95it/s]\n",
            "Steps:  47%|█▍ | 644/1380 [10:12<03:40,  3.34it/s, lr=0.0001, step_loss=0.00798]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 15.27it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00, 10.10it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'norm_num_groups', 'use_quant_conv', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 16.85it/s]\n",
            "Steps:  50%|██▌  | 690/1380 [10:58<03:28,  3.31it/s, lr=0.0001, step_loss=0.311]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 15.45it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00, 10.36it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'norm_num_groups', 'use_quant_conv', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 17.30it/s]\n",
            "Steps:  53%|██▋  | 736/1380 [11:44<03:13,  3.33it/s, lr=0.0001, step_loss=0.018]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 14.55it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00, 10.05it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'norm_num_groups', 'use_quant_conv', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 16.71it/s]\n",
            "Steps:  57%|██▊  | 782/1380 [12:31<02:59,  3.34it/s, lr=0.0001, step_loss=0.344]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 15.13it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00, 10.14it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'norm_num_groups', 'use_quant_conv', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 16.92it/s]\n",
            "Steps:  60%|██▍ | 828/1380 [13:17<02:45,  3.34it/s, lr=0.0001, step_loss=0.0098]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 14.13it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00,  4.74it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'norm_num_groups', 'use_quant_conv', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00,  8.74it/s]\n",
            "Steps:  63%|██▌ | 874/1380 [14:04<02:32,  3.32it/s, lr=0.0001, step_loss=0.0568]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 14.84it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00, 10.13it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'norm_num_groups', 'use_quant_conv', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 16.85it/s]\n",
            "Steps:  67%|███▎ | 920/1380 [14:50<02:18,  3.32it/s, lr=0.0001, step_loss=0.044]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 15.25it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00, 10.35it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'norm_num_groups', 'use_quant_conv', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 17.18it/s]\n",
            "Steps:  70%|██▊ | 966/1380 [15:36<02:04,  3.31it/s, lr=0.0001, step_loss=0.0638]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 14.92it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00, 10.16it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'norm_num_groups', 'use_quant_conv', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 16.89it/s]\n",
            "Steps:  73%|███▋ | 1012/1380 [16:23<01:50,  3.32it/s, lr=0.0001, step_loss=0.19]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 15.03it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00, 10.09it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'norm_num_groups', 'use_quant_conv', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 16.79it/s]\n",
            "Steps:  77%|██▎| 1058/1380 [17:09<01:36,  3.34it/s, lr=0.0001, step_loss=0.0695]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 14.86it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00, 10.07it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'norm_num_groups', 'use_quant_conv', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 16.78it/s]\n",
            "Steps:  80%|███▏| 1104/1380 [17:56<01:22,  3.34it/s, lr=0.0001, step_loss=0.193]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 15.17it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00, 10.25it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'norm_num_groups', 'use_quant_conv', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 17.00it/s]\n",
            "Steps:  83%|███▎| 1150/1380 [18:42<01:08,  3.34it/s, lr=0.0001, step_loss=0.437]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 14.61it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00, 10.00it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'norm_num_groups', 'use_quant_conv', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 16.67it/s]\n",
            "Steps:  87%|██▌| 1196/1380 [19:28<00:55,  3.33it/s, lr=0.0001, step_loss=0.0649]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 14.74it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00,  9.99it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'norm_num_groups', 'use_quant_conv', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 16.60it/s]\n",
            "Steps:  90%|███▌| 1242/1380 [20:14<00:41,  3.34it/s, lr=0.0001, step_loss=0.041]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 14.25it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00,  9.95it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'norm_num_groups', 'use_quant_conv', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 16.53it/s]\n",
            "Steps:  93%|██▊| 1288/1380 [21:01<00:27,  3.33it/s, lr=0.0001, step_loss=0.0033]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 14.85it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00, 10.08it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'norm_num_groups', 'use_quant_conv', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 16.75it/s]\n",
            "Steps:  97%|███▊| 1334/1380 [21:47<00:13,  3.33it/s, lr=0.0001, step_loss=0.257]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 14.83it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00,  9.99it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'norm_num_groups', 'use_quant_conv', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 16.69it/s]\n",
            "Steps: 100%|████| 1380/1380 [22:33<00:00,  3.36it/s, lr=0.0001, step_loss=0.659]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 14.93it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00, 10.08it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'norm_num_groups', 'use_quant_conv', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 16.72it/s]\n",
            "Model weights saved in /content/sd-flower22-model-lora/pytorch_lora_weights.safetensors\n",
            "{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 15.24it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00, 10.16it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float32.\n",
            "{'use_post_quant_conv', 'norm_num_groups', 'use_quant_conv', 'mid_block_add_attention', 'latents_std', 'latents_mean', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/vae.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Instantiating UNet2DConditionModel model under default dtype torch.float32.\n",
            "{'time_cond_proj_dim', 'mid_block_only_cross_attention', 'transformer_layers_per_block', 'only_cross_attention', 'resnet_out_scale_factor', 'conv_out_kernel', 'encoder_hid_dim_type', 'use_linear_projection', 'class_embed_type', 'dropout', 'projection_class_embeddings_input_dim', 'conv_in_kernel', 'resnet_skip_time_act', 'class_embeddings_concat', 'encoder_hid_dim', 'resnet_time_scale_shift', 'attention_type', 'num_attention_heads', 'cross_attention_norm', 'num_class_embeds', 'time_embedding_act_fn', 'mid_block_type', 'time_embedding_dim', 'timestep_post_act', 'addition_embed_type_num_heads', 'addition_embed_type', 'addition_time_embed_dim', 'upcast_attention', 'dual_cross_attention', 'reverse_transformer_layers_per_block', 'time_embedding_type'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
            "\n",
            "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/unet.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
            "Loaded unet as UNet2DConditionModel from `unet` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  86%|███████████▏ | 6/7 [00:00<00:00, 11.34it/s]\u001b[A{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 13.26it/s]\n",
            "Loading unet.\n",
            "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
            "Steps: 100%|████| 1380/1380 [23:40<00:00,  1.03s/it, lr=0.0001, step_loss=0.659]\n"
          ]
        }
      ],
      "source": [
        "!accelerate launch  train_text_to_image_lora.py \\\n",
        "  --pretrained_model_name_or_path=\"CompVis/stable-diffusion-v1-4\" \\\n",
        "  --dataset_name=\"AhmadMustafa/flower-captions-blip\" --caption_column=\"text\" \\\n",
        "  --resolution=256 --random_flip \\\n",
        "  --train_batch_size=1 \\\n",
        "  --num_train_epochs=30 --checkpointing_steps=5000 \\\n",
        "  --learning_rate=1e-04 --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n",
        "  --seed=42 \\\n",
        "  --output_dir=\"/content/sd-flower22-model-lora\" \\\n",
        "  --validation_prompt=\"beautiful flower\" --report_to=\"tensorboard\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-10T17:50:39.374174Z",
          "iopub.status.busy": "2025-04-10T17:50:39.373870Z",
          "iopub.status.idle": "2025-04-10T17:50:41.606093Z",
          "shell.execute_reply": "2025-04-10T17:50:41.605172Z",
          "shell.execute_reply.started": "2025-04-10T17:50:39.374149Z"
        },
        "id": "-DOMA9DPcHpa",
        "outputId": "0dd59af4-443e-46f8-e62e-42d02db844bc",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: content/sd-flower22-model-lora/ (stored 0%)\n",
            "  adding: content/sd-flower22-model-lora/logs/ (stored 0%)\n",
            "  adding: content/sd-flower22-model-lora/logs/text2image-fine-tune/ (stored 0%)\n",
            "  adding: content/sd-flower22-model-lora/logs/text2image-fine-tune/1744306002.8833723/ (stored 0%)\n",
            "  adding: content/sd-flower22-model-lora/logs/text2image-fine-tune/1744306002.8833723/events.out.tfevents.1744306002.c6822e3741df.144.1 (deflated 52%)\n",
            "  adding: content/sd-flower22-model-lora/logs/text2image-fine-tune/1744306002.8853486/ (stored 0%)\n",
            "  adding: content/sd-flower22-model-lora/logs/text2image-fine-tune/1744306002.8853486/hparams.yml (deflated 48%)\n",
            "  adding: content/sd-flower22-model-lora/logs/text2image-fine-tune/events.out.tfevents.1744306002.c6822e3741df.144.0 (deflated 0%)\n",
            "  adding: content/sd-flower22-model-lora/pytorch_lora_weights.safetensors (deflated 6%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r /kaggle/working/sd_flower_model-lora.zip /content/sd-flower22-model-lora/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31011,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
